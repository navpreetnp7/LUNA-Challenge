{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from collections import namedtuple\n",
    "import copy\n",
    "import csv\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Some libraries attempt to add their own root logger handlers. This is\n",
    "# annoying and so we get rid of them.\n",
    "for handler in list(root_logger.handlers):\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n",
    "formatter = logging.Formatter(logfmt_str)\n",
    "\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "streamHandler.setLevel(logging.DEBUG)\n",
    "\n",
    "root_logger.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger(__name__)\n",
    "# log.setLevel(logging.WARN)\n",
    "# log.setLevel(logging.INFO)\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache import FanoutCache, Disk\n",
    "from diskcache.core import BytesType, MODE_BINARY, BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCache(scope_str):\n",
    "    return FanoutCache('../data-luna/data-unversioned/cache/' + scope_str,\n",
    "                       disk=GzipDisk,\n",
    "                       shards=64,\n",
    "                       timeout=1,\n",
    "                       size_limit=3e11,\n",
    "                       # disk_min_file_size=2**20,\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GzipDisk(Disk):\n",
    "    def store(self, value, read, key=None):\n",
    "        \"\"\"\n",
    "        Override from base class diskcache.Disk.\n",
    "        Chunking is due to needing to work on pythons < 2.7.13:\n",
    "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
    "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
    "          compression and decompression operations did not properly handle results of\n",
    "          2 or 4 GiB.\n",
    "        :param value: value to convert\n",
    "        :param bool read: True when value is file-like object\n",
    "        :return: (size, mode, filename, value) tuple for Cache table\n",
    "        \"\"\"\n",
    "        # pylint: disable=unidiomatic-typecheck\n",
    "        if type(value) is BytesType:\n",
    "            if read:\n",
    "                value = value.read()\n",
    "                read = False\n",
    "\n",
    "            str_io = BytesIO()\n",
    "            gz_file = gzip.GzipFile(mode='wb', compresslevel=1, fileobj=str_io)\n",
    "\n",
    "            for offset in range(0, len(value), 2**30):\n",
    "                gz_file.write(value[offset:offset+2**30])\n",
    "            gz_file.close()\n",
    "\n",
    "            value = str_io.getvalue()\n",
    "\n",
    "        return super(GzipDisk, self).store(value, read)\n",
    "\n",
    "\n",
    "    def fetch(self, mode, filename, value, read):\n",
    "        \"\"\"\n",
    "        Override from base class diskcache.Disk.\n",
    "        Chunking is due to needing to work on pythons < 2.7.13:\n",
    "        - Issue #27130: In the \"zlib\" module, fix handling of large buffers\n",
    "          (typically 2 or 4 GiB).  Previously, inputs were limited to 2 GiB, and\n",
    "          compression and decompression operations did not properly handle results of\n",
    "          2 or 4 GiB.\n",
    "        :param int mode: value mode raw, binary, text, or pickle\n",
    "        :param str filename: filename of corresponding value\n",
    "        :param value: database value\n",
    "        :param bool read: when True, return an open file handle\n",
    "        :return: corresponding Python value\n",
    "        \"\"\"\n",
    "        value = super(GzipDisk, self).fetch(mode, filename, value, read)\n",
    "\n",
    "        if mode == MODE_BINARY:\n",
    "            str_io = BytesIO(value)\n",
    "            gz_file = gzip.GzipFile(mode='rb', fileobj=str_io)\n",
    "            read_csio = BytesIO()\n",
    "\n",
    "            while True:\n",
    "                uncompressed_data = gz_file.read(2**30)\n",
    "                if uncompressed_data:\n",
    "                    read_csio.write(uncompressed_data)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            value = read_csio.getvalue()\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IrcTuple = collections.namedtuple('IrcTuple', ['index', 'row', 'col'])\n",
    "XyzTuple = collections.namedtuple('XyzTuple', ['x', 'y', 'z'])\n",
    "\n",
    "def irc2xyz(coord_irc, origin_xyz, vxSize_xyz, direction_a):\n",
    "    cri_a = np.array(coord_irc)[::-1]\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coords_xyz = (direction_a @ (cri_a * vxSize_a)) + origin_a\n",
    "    # coords_xyz = (direction_a @ (idx * vxSize_a)) + origin_a\n",
    "    return XyzTuple(*coords_xyz)\n",
    "\n",
    "def xyz2irc(coord_xyz, origin_xyz, vxSize_xyz, direction_a):\n",
    "    origin_a = np.array(origin_xyz)\n",
    "    vxSize_a = np.array(vxSize_xyz)\n",
    "    coord_a = np.array(coord_xyz)\n",
    "    cri_a = ((coord_a - origin_a) @ np.linalg.inv(direction_a)) / vxSize_a\n",
    "    cri_a = np.round(cri_a)\n",
    "    return IrcTuple(int(cri_a[2]), int(cri_a[1]), int(cri_a[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cache = getCache('raw_dset')\n",
    "\n",
    "CandidateInfoTuple = namedtuple(\n",
    "    'CandidateInfoTuple',\n",
    "    'isNodule_bool, diameter_mm, series_uid, center_xyz',\n",
    ")\n",
    "\n",
    "@functools.lru_cache(1)\n",
    "def getCandidateInfoList(requireOnDisk_bool=True):\n",
    "    # We construct a set with all series_uids that are present on disk.\n",
    "    # This will let us use the data, even if we haven't downloaded all of\n",
    "    # the subsets yet.\n",
    "    mhd_list = glob.glob('data-unversioned/part2/luna/subset*/*.mhd')\n",
    "    presentOnDisk_set = {os.path.split(p)[-1][:-4] for p in mhd_list}\n",
    "\n",
    "    diameter_dict = {}\n",
    "    with open('data/part2/luna/annotations.csv', \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "            annotationCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "            annotationDiameter_mm = float(row[4])\n",
    "\n",
    "            diameter_dict.setdefault(series_uid, []).append(\n",
    "                (annotationCenter_xyz, annotationDiameter_mm),\n",
    "            )\n",
    "\n",
    "    candidateInfo_list = []\n",
    "    with open('data/part2/luna/candidates.csv', \"r\") as f:\n",
    "        for row in list(csv.reader(f))[1:]:\n",
    "            series_uid = row[0]\n",
    "\n",
    "            if series_uid not in presentOnDisk_set and requireOnDisk_bool:\n",
    "                continue\n",
    "\n",
    "            isNodule_bool = bool(int(row[4]))\n",
    "            candidateCenter_xyz = tuple([float(x) for x in row[1:4]])\n",
    "\n",
    "            candidateDiameter_mm = 0.0\n",
    "            for annotation_tup in diameter_dict.get(series_uid, []):\n",
    "                annotationCenter_xyz, annotationDiameter_mm = annotation_tup\n",
    "                for i in range(3):\n",
    "                    delta_mm = abs(candidateCenter_xyz[i] - annotationCenter_xyz[i])\n",
    "                    if delta_mm > annotationDiameter_mm / 4:\n",
    "                        break\n",
    "                else:\n",
    "                    candidateDiameter_mm = annotationDiameter_mm\n",
    "                    break\n",
    "\n",
    "            candidateInfo_list.append(CandidateInfoTuple(\n",
    "                isNodule_bool,\n",
    "                candidateDiameter_mm,\n",
    "                series_uid,\n",
    "                candidateCenter_xyz,\n",
    "            ))\n",
    "\n",
    "    candidateInfo_list.sort(reverse=True)\n",
    "    return candidateInfo_list\n",
    "\n",
    "class Ct:\n",
    "    def __init__(self, series_uid):\n",
    "        mhd_path = glob.glob(\n",
    "            'data-unversioned/part2/luna/subset*/{}.mhd'.format(series_uid)\n",
    "        )[0]\n",
    "\n",
    "        ct_mhd = sitk.ReadImage(mhd_path)\n",
    "        ct_a = np.array(sitk.GetArrayFromImage(ct_mhd), dtype=np.float32)\n",
    "\n",
    "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
    "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
    "        # The upper bound nukes any weird hotspots and clamps bone down\n",
    "        ct_a.clip(-1000, 1000, ct_a)\n",
    "\n",
    "        self.series_uid = series_uid\n",
    "        self.hu_a = ct_a\n",
    "\n",
    "        self.origin_xyz = XyzTuple(*ct_mhd.GetOrigin())\n",
    "        self.vxSize_xyz = XyzTuple(*ct_mhd.GetSpacing())\n",
    "        self.direction_a = np.array(ct_mhd.GetDirection()).reshape(3, 3)\n",
    "\n",
    "    def getRawCandidate(self, center_xyz, width_irc):\n",
    "        center_irc = xyz2irc(\n",
    "            center_xyz,\n",
    "            self.origin_xyz,\n",
    "            self.vxSize_xyz,\n",
    "            self.direction_a,\n",
    "        )\n",
    "\n",
    "        slice_list = []\n",
    "        for axis, center_val in enumerate(center_irc):\n",
    "            start_ndx = int(round(center_val - width_irc[axis]/2))\n",
    "            end_ndx = int(start_ndx + width_irc[axis])\n",
    "\n",
    "            assert center_val >= 0 and center_val < self.hu_a.shape[axis], repr([self.series_uid, center_xyz, self.origin_xyz, self.vxSize_xyz, center_irc, axis])\n",
    "\n",
    "            if start_ndx < 0:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                start_ndx = 0\n",
    "                end_ndx = int(width_irc[axis])\n",
    "\n",
    "            if end_ndx > self.hu_a.shape[axis]:\n",
    "                # log.warning(\"Crop outside of CT array: {} {}, center:{} shape:{} width:{}\".format(\n",
    "                #     self.series_uid, center_xyz, center_irc, self.hu_a.shape, width_irc))\n",
    "                end_ndx = self.hu_a.shape[axis]\n",
    "                start_ndx = int(self.hu_a.shape[axis] - width_irc[axis])\n",
    "\n",
    "            slice_list.append(slice(start_ndx, end_ndx))\n",
    "\n",
    "        ct_chunk = self.hu_a[tuple(slice_list)]\n",
    "\n",
    "        return ct_chunk, center_irc\n",
    "\n",
    "\n",
    "@functools.lru_cache(1, typed=True)\n",
    "def getCt(series_uid):\n",
    "    return Ct(series_uid)\n",
    "\n",
    "@raw_cache.memoize(typed=True)\n",
    "def getCtRawCandidate(series_uid, center_xyz, width_irc):\n",
    "    ct = getCt(series_uid)\n",
    "    ct_chunk, center_irc = ct.getRawCandidate(center_xyz, width_irc)\n",
    "    return ct_chunk, center_irc\n",
    "\n",
    "\n",
    "class LunaDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 val_stride=0,\n",
    "                 isValSet_bool=None,\n",
    "                 series_uid=None,\n",
    "                 sortby_str='random',\n",
    "            ):\n",
    "        self.candidateInfo_list = copy.copy(getCandidateInfoList())\n",
    "\n",
    "        if series_uid:\n",
    "            self.candidateInfo_list = [\n",
    "                x for x in self.candidateInfo_list if x.series_uid == series_uid\n",
    "            ]\n",
    "\n",
    "        if isValSet_bool:\n",
    "            assert val_stride > 0, val_stride\n",
    "            self.candidateInfo_list = self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "        elif val_stride > 0:\n",
    "            del self.candidateInfo_list[::val_stride]\n",
    "            assert self.candidateInfo_list\n",
    "\n",
    "        if sortby_str == 'random':\n",
    "            random.shuffle(self.candidateInfo_list)\n",
    "        elif sortby_str == 'series_uid':\n",
    "            self.candidateInfo_list.sort(key=lambda x: (x.series_uid, x.center_xyz))\n",
    "        elif sortby_str == 'label_and_size':\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Unknown sort: \" + repr(sortby_str))\n",
    "\n",
    "        log.info(\"{!r}: {} {} samples\".format(\n",
    "            self,\n",
    "            len(self.candidateInfo_list),\n",
    "            \"validation\" if isValSet_bool else \"training\",\n",
    "        ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.candidateInfo_list)\n",
    "\n",
    "    def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.candidateInfo_list[ndx]\n",
    "        width_irc = (32, 48, 48)\n",
    "\n",
    "        candidate_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            width_irc,\n",
    "        )\n",
    "        candidate_t = torch.from_numpy(candidate_a).to(torch.float32)\n",
    "        candidate_t = candidate_t.unsqueeze(0)\n",
    "\n",
    "        pos_t = torch.tensor([\n",
    "                not candidateInfo_tup.isNodule_bool,\n",
    "                candidateInfo_tup.isNodule_bool\n",
    "            ],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        return candidate_t, pos_t, candidateInfo_tup.series_uid, torch.tensor(center_irc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaPrepCacheApp:\n",
    "    @classmethod\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--batch-size',\n",
    "            help='Batch size to use for training',\n",
    "            default=1024,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        self.prep_dl = DataLoader(\n",
    "            LunaDataset(\n",
    "                sortby_str='series_uid',\n",
    "            ),\n",
    "            batch_size=self.cli_args.batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            self.prep_dl,\n",
    "            \"Stuffing cache\",\n",
    "            start_ndx=self.prep_dl.num_workers,\n",
    "        )\n",
    "        for _ in batch_iter:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LunaPrepCacheApp().main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, conv_channels=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
    "\n",
    "        self.block1 = LunaBlock(in_channels, conv_channels)\n",
    "        self.block2 = LunaBlock(conv_channels, conv_channels * 2)\n",
    "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
    "        self.block4 = LunaBlock(conv_channels * 4, conv_channels * 8)\n",
    "\n",
    "        self.head_linear = nn.Linear(1152, 2)\n",
    "        self.head_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    # see also https://github.com/pytorch/pytorch/issues/18182\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) in {\n",
    "                nn.Linear,\n",
    "                nn.Conv3d,\n",
    "                nn.Conv2d,\n",
    "                nn.ConvTranspose2d,\n",
    "                nn.ConvTranspose3d,\n",
    "            }:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = \\\n",
    "                        nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.tail_batchnorm(input_batch)\n",
    "\n",
    "        block_out = self.block1(bn_output)\n",
    "        block_out = self.block2(block_out)\n",
    "        block_out = self.block3(block_out)\n",
    "        block_out = self.block4(block_out)\n",
    "\n",
    "        conv_flat = block_out.view(\n",
    "            block_out.size(0),\n",
    "            -1,\n",
    "        )\n",
    "        linear_output = self.head_linear(conv_flat)\n",
    "\n",
    "        return linear_output, self.head_softmax(linear_output)\n",
    "\n",
    "\n",
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size=3, padding=1, bias=True,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2, 2)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "\n",
    "        return self.maxpool(block_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for computeBatchLoss and logMetrics to index into metrics_t/metrics_a\n",
    "METRICS_LABEL_NDX=0\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_LOSS_NDX=2\n",
    "METRICS_SIZE = 3\n",
    "\n",
    "class LunaTrainingApp:\n",
    "    def __init__(self, sys_argv=None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "            help='Number of worker processes for background data loading',\n",
    "            default=8,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--batch-size',\n",
    "            help='Batch size to use for training',\n",
    "            default=32,\n",
    "            type=int,\n",
    "        )\n",
    "        parser.add_argument('--epochs',\n",
    "            help='Number of epochs to train for',\n",
    "            default=1,\n",
    "            type=int,\n",
    "        )\n",
    "\n",
    "        parser.add_argument('--tb-prefix',\n",
    "            default='p2ch11',\n",
    "            help=\"Data prefix to use for Tensorboard run. Defaults to chapter.\",\n",
    "        )\n",
    "\n",
    "        parser.add_argument('comment',\n",
    "            help=\"Comment suffix for Tensorboard run.\",\n",
    "            nargs='?',\n",
    "            default='dwlpt',\n",
    "        )\n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "\n",
    "        self.trn_writer = None\n",
    "        self.val_writer = None\n",
    "        self.totalTrainingSamples_count = 0\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "\n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "\n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "            model = model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    def initOptimizer(self):\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "        # return Adam(self.model.parameters())\n",
    "\n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=False,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return train_dl\n",
    "\n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return val_dl\n",
    "\n",
    "    def initTensorboardWriters(self):\n",
    "        if self.trn_writer is None:\n",
    "            log_dir = os.path.join('runs', self.cli_args.tb_prefix, self.time_str)\n",
    "\n",
    "            self.trn_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '-trn_cls-' + self.cli_args.comment)\n",
    "            self.val_writer = SummaryWriter(\n",
    "                log_dir=log_dir + '-val_cls-' + self.cli_args.comment)\n",
    "\n",
    "\n",
    "    def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "\n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "\n",
    "            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            ))\n",
    "\n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "\n",
    "        if hasattr(self, 'trn_writer'):\n",
    "            self.trn_writer.close()\n",
    "            self.val_writer.close()\n",
    "\n",
    "\n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        self.model.train()\n",
    "        trnMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(train_dl.dataset),\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx=train_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss_var = self.computeBatchLoss(\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                train_dl.batch_size,\n",
    "                trnMetrics_g\n",
    "            )\n",
    "\n",
    "            loss_var.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # # This is for adding the model graph to TensorBoard.\n",
    "            # if epoch_ndx == 1 and batch_ndx == 0:\n",
    "            #     with torch.no_grad():\n",
    "            #         model = LunaModel()\n",
    "            #         self.trn_writer.add_graph(model, batch_tup[0], verbose=True)\n",
    "            #         self.trn_writer.close()\n",
    "\n",
    "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "\n",
    "        return trnMetrics_g.to('cpu')\n",
    "\n",
    "\n",
    "    def doValidation(self, epoch_ndx, val_dl):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            valMetrics_g = torch.zeros(\n",
    "                METRICS_SIZE,\n",
    "                len(val_dl.dataset),\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                val_dl,\n",
    "                \"E{} Validation \".format(epoch_ndx),\n",
    "                start_ndx=val_dl.num_workers,\n",
    "            )\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(\n",
    "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "        return valMetrics_g.to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "\n",
    "        input_g = input_t.to(self.device, non_blocking=True)\n",
    "        label_g = label_t.to(self.device, non_blocking=True)\n",
    "\n",
    "        logits_g, probability_g = self.model(input_g)\n",
    "\n",
    "        loss_func = nn.CrossEntropyLoss(reduction='none')\n",
    "        loss_g = loss_func(\n",
    "            logits_g,\n",
    "            label_g[:,1],\n",
    "        )\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + label_t.size(0)\n",
    "\n",
    "        metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = \\\n",
    "            label_g[:,1].detach()\n",
    "        metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = \\\n",
    "            probability_g[:,1].detach()\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = \\\n",
    "            loss_g.detach()\n",
    "\n",
    "        return loss_g.mean()\n",
    "\n",
    "\n",
    "    def logMetrics(\n",
    "            self,\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            metrics_t,\n",
    "            classificationThreshold=0.5,\n",
    "    ):\n",
    "        self.initTensorboardWriters()\n",
    "        log.info(\"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            type(self).__name__,\n",
    "        ))\n",
    "\n",
    "        negLabel_mask = metrics_t[METRICS_LABEL_NDX] <= classificationThreshold\n",
    "        negPred_mask = metrics_t[METRICS_PRED_NDX] <= classificationThreshold\n",
    "\n",
    "        posLabel_mask = ~negLabel_mask\n",
    "        posPred_mask = ~negPred_mask\n",
    "\n",
    "        neg_count = int(negLabel_mask.sum())\n",
    "        pos_count = int(posLabel_mask.sum())\n",
    "\n",
    "        neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "        pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "\n",
    "        metrics_dict = {}\n",
    "        metrics_dict['loss/all'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX].mean()\n",
    "        metrics_dict['loss/neg'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "        metrics_dict['loss/pos'] = \\\n",
    "            metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "\n",
    "        metrics_dict['correct/all'] = (pos_correct + neg_correct) \\\n",
    "            / np.float32(metrics_t.shape[1]) * 100\n",
    "        metrics_dict['correct/neg'] = neg_correct / np.float32(neg_count) * 100\n",
    "        metrics_dict['correct/pos'] = pos_correct / np.float32(pos_count) * 100\n",
    "\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/all:.4f} loss, \"\n",
    "                 + \"{correct/all:-5.1f}% correct, \"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/neg:.4f} loss, \"\n",
    "                 + \"{correct/neg:-5.1f}% correct ({neg_correct:} of {neg_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_neg',\n",
    "                neg_correct=neg_correct,\n",
    "                neg_count=neg_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "        log.info(\n",
    "            (\"E{} {:8} {loss/pos:.4f} loss, \"\n",
    "                 + \"{correct/pos:-5.1f}% correct ({pos_correct:} of {pos_count:})\"\n",
    "            ).format(\n",
    "                epoch_ndx,\n",
    "                mode_str + '_pos',\n",
    "                pos_correct=pos_correct,\n",
    "                pos_count=pos_count,\n",
    "                **metrics_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        writer = getattr(self, mode_str + '_writer')\n",
    "\n",
    "        for key, value in metrics_dict.items():\n",
    "            writer.add_scalar(key, value, self.totalTrainingSamples_count)\n",
    "\n",
    "        writer.add_pr_curve(\n",
    "            'pr',\n",
    "            metrics_t[METRICS_LABEL_NDX],\n",
    "            metrics_t[METRICS_PRED_NDX],\n",
    "            self.totalTrainingSamples_count,\n",
    "        )\n",
    "\n",
    "        bins = [x/50.0 for x in range(51)]\n",
    "\n",
    "        negHist_mask = negLabel_mask & (metrics_t[METRICS_PRED_NDX] > 0.01)\n",
    "        posHist_mask = posLabel_mask & (metrics_t[METRICS_PRED_NDX] < 0.99)\n",
    "\n",
    "        if negHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_neg',\n",
    "                metrics_t[METRICS_PRED_NDX, negHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n",
    "        if posHist_mask.any():\n",
    "            writer.add_histogram(\n",
    "                'is_pos',\n",
    "                metrics_t[METRICS_PRED_NDX, posHist_mask],\n",
    "                self.totalTrainingSamples_count,\n",
    "                bins=bins,\n",
    "            )\n",
    "\n",
    "        # score = 1 \\\n",
    "        #     + metrics_dict['pr/f1_score'] \\\n",
    "        #     - metrics_dict['loss/mal'] * 0.01 \\\n",
    "        #     - metrics_dict['loss/all'] * 0.0001\n",
    "        #\n",
    "        # return score\n",
    "\n",
    "    # def logModelMetrics(self, model):\n",
    "    #     writer = getattr(self, 'trn_writer')\n",
    "    #\n",
    "    #     model = getattr(model, 'module', model)\n",
    "    #\n",
    "    #     for name, param in model.named_parameters():\n",
    "    #         if param.requires_grad:\n",
    "    #             min_data = float(param.data.min())\n",
    "    #             max_data = float(param.data.max())\n",
    "    #             max_extent = max(abs(min_data), abs(max_data))\n",
    "    #\n",
    "    #             # bins = [x/50*max_extent for x in range(-50, 51)]\n",
    "    #\n",
    "    #             try:\n",
    "    #                 writer.add_histogram(\n",
    "    #                     name.rsplit('.', 1)[-1] + '/' + name,\n",
    "    #                     param.data.cpu().numpy(),\n",
    "    #                     # metrics_a[METRICS_PRED_NDX, negHist_mask],\n",
    "    #                     self.totalTrainingSamples_count,\n",
    "    #                     # bins=bins,\n",
    "    #                 )\n",
    "    #             except Exception as e:\n",
    "    #                 log.error([min_data, max_data])\n",
    "    #                 raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
